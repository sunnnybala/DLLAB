{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n",
      "True\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0))  # Should display your GPU name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. 1. Draw Computation Graph and work out the gradient dz/da  by following the path \n",
    "back from z to a and compare the result with the analytical gradient.   \n",
    "x = 2*a + 3*b \n",
    "y = 5*a*a + 3*b*b*b \n",
    "z = 2*x + 3*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient at a:  tensor(64.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(4.0, requires_grad=True) \n",
    "x =  2*a +3*b\n",
    "y = 5*a*a + 3*b*b*b\n",
    "z = 2*x +3*y\n",
    "z.backward()\n",
    "print(\"gradient at a: \", a.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q2) For the following Computation Graph, work out the gradient da/dw  by following the \n",
    "path back from a to w and compare the result with the analytical gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16., grad_fn=<MulBackward0>)\n",
      "gradient at w:  tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(4.0, requires_grad=True) \n",
    "w = torch.tensor(6.0, requires_grad=True) \n",
    "u = w*x\n",
    "v=(u+b) *1\n",
    "if max(v,0) !=0:\n",
    "    a =v\n",
    "else:\n",
    "    a = torch.tensor(0.0, requires_grad=True) \n",
    "#a = torch.relu(v)\n",
    "print(a)\n",
    "a.backward()\n",
    "print(\"gradient at w: \", w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytical sol:  2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def manual_q2(x,w,b):\n",
    "    u = w*x\n",
    "    v=(u+b) *1\n",
    "    if max(v,0) !=0:\n",
    "        a =v\n",
    "        dadv = 1\n",
    "    else:\n",
    "        a = torch.tensor(0.0, requires_grad=True)\n",
    "        dadv= 0\n",
    "    dvdu=1\n",
    "    dvdb =1\n",
    "    dudw=x\n",
    "    dudx = w\n",
    "    temp = dadv*dvdu*dudw\n",
    "    return temp\n",
    "print(\"analytical sol: \", manual_q2(2,4,6))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Repeat the Problem 2 using Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6637, grad_fn=<MulBackward0>)\n",
      "gradient at w:  tensor(0.0446)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(0.20, requires_grad=True)\n",
    "w = torch.tensor(0.40, requires_grad=True) \n",
    "b = torch.tensor(0.60, requires_grad=True) \n",
    "u = w*x\n",
    "v=(u+b) *1\n",
    "a =1/(1+torch.exp(-1*v))\n",
    "#a = torch.sigmoid(v)\n",
    "print(a)\n",
    "a.backward()\n",
    "print(\"gradient at w: \", w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytical sol:  0.04463792779446517\n"
     ]
    }
   ],
   "source": [
    "def manual_q3(x,w,b):\n",
    "    u = w*x\n",
    "    v=(u+b) *1\n",
    "    a = 1 / (1 + math.exp(-v))\n",
    "    dadv = a*(1-a)\n",
    "    dvdu=1\n",
    "    dvdb =1\n",
    "    dudw=x\n",
    "    dudx = w\n",
    "    temp = dadv*dvdu*dudw\n",
    "    return temp\n",
    "print(\"analytical sol: \", manual_q3(0.2,0.4,0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Verify that the gradients provided by PyTorch match with the analytical gradients of \n",
    "the function f= exp(-x2-2x-sin(x))  w.r.t x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, grad_fn=<ExpBackward0>)\n",
      "gradient at x:  -0.0007545282132923603\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "a = torch.exp((-1*x*x)-(2*x)-torch.sin(x))\n",
    "print(a)\n",
    "a.backward()\n",
    "print(\"gradient at x: \", x.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytical sol:  -0.0007545278582400674\n"
     ]
    }
   ],
   "source": [
    "def manual_q4(x):\n",
    "    u = -1*x*x\n",
    "    v= -2*x\n",
    "    p = math.sin(x)\n",
    "    q= u+v-p\n",
    "    a = math.exp(q)\n",
    "    dadq = math.exp(q)\n",
    "    dqdu=1\n",
    "    dqdv=1\n",
    "    dqdp=-1\n",
    "    dudx = -2*x\n",
    "    dvdx=-2\n",
    "    dpdx=math.cos(x)\n",
    "    \n",
    "    temp = (dadq*dqdu*dudx)+(dadq*dqdv*dvdx)+(dadq*dqdp*dpdx)\n",
    "    return temp\n",
    "print(\"analytical sol: \", manual_q4(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compute gradient for the function y=8x4+ 3x3 +7x2+6x+3 and verify the gradients \n",
    "provided by PyTorch with the analytical gradients. A snapshot of the Python code is \n",
    "provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(195., grad_fn=<AddBackward0>)\n",
      "gradient at x:  326.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "a = 8*x**4 + 3*x**3 + 7*x**2 + 6*x +3\n",
    "print(a)\n",
    "a.backward()\n",
    "print(\"gradient at x: \", x.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytical sol:  326.0\n"
     ]
    }
   ],
   "source": [
    "def manual_q4(x):\n",
    "    u = 8*x**4\n",
    "    v= 3*x**3\n",
    "    p = 7*x**2\n",
    "    q= 6*x\n",
    "    a = u+v+p+q+3\n",
    "    dadq = 1\n",
    "    dadu=1\n",
    "    dadv=1\n",
    "    dadp=1\n",
    "    dqdx=6\n",
    "    dpdx=14*x\n",
    "    dvdx=9*x**2\n",
    "    dudx = 32*x**3\n",
    "    \n",
    "    temp = (dadq*dqdx)+(dadv*dvdx)+(dadp*dpdx)+(dadu*dudx)\n",
    "    return temp\n",
    "print(\"analytical sol: \", manual_q4(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. For the following function, computation graph is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient at y:  -0.005758530460298061\n",
      "gradient at a:  0.00024393710191361606\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.6, requires_grad=True)\n",
    "y = torch.tensor(6.5, requires_grad=True) \n",
    "z = torch.tensor(2.26, requires_grad=True)\n",
    "a =  2*x\n",
    "a.retain_grad()\n",
    "b = torch.sin(y)\n",
    "c = a/b\n",
    "d = c*z\n",
    "e = torch.log(d+1)\n",
    "f = torch.tanh(e)\n",
    "f.backward()\n",
    "print(\"gradient at y: \", y.grad.item())\n",
    "print(\"gradient at a: \", a.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytical solution:  -0.005758530460298061\n"
     ]
    }
   ],
   "source": [
    "def calc_grad(x, y, z, a, b, c, d, e, f):\n",
    "    dfde = 1-torch.tanh(e)**2\n",
    "    dedd = 1/(d+1); dddc = z\n",
    "    dcdb = -1*a/b**2\n",
    "    dbdy = torch.cos(y)\n",
    "    dfdy = dfde*dedd*dddc*dcdb*dbdy\n",
    "    return dfdy.item()\n",
    "print(\"analytical solution: \", calc_grad(x,y,z,a,b,c,d,e,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
